{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "XN3SPNk4WVzj",
        "gypmYH_ekAeb",
        "ING_DNl6gcN3",
        "GdDWbWZyla4U",
        "Am3uBnVklygG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Intro\n",
        "\n",
        "This notebook is only used to train the network, the data generation is done in the python scripts that are provided in the github. In order to be able to run the whole notebook you need to have a folder called data with the files hits.txt and parameters.txt inside. To be able to save the model you need a folder named model.\n",
        "\n",
        "```\n",
        "- notebook.ipynb\n",
        "- data\n",
        "  - parameters.txt\n",
        "  - hits.txt\n",
        "- model\n",
        "```\n",
        "\n",
        "We start by doing all of our imports and defining some constants. Then we load the device in order to be able to use the GPU, and we load the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "dsd6qqwFHfEp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "rkGuxqY7u_NK"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import torch\n",
        "from torch import nn\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch import Tensor\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from timeit import default_timer as timer\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import matplotlib.pyplot as plt\n",
        "# from utils import custom_collate, create_mask_src, create_output_pred_mask, sort_by_angle, load_variable_len_data, get_labels_2d, get_labels_3d, normalize_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DETECTORS = [1, 2, 3, 4, 5]\n",
        "NR_DETECTORS = len(DETECTORS)\n",
        "DIM = 2\n",
        "DATA_FILENAME = \"data/hits.txt\"\n",
        "LABEL_FILENAME = \"data/parameters.txt\"\n",
        "BATCH_SIZE = 32\n",
        "TEST_BATCH_SIZE = 2\n",
        "PADDING_LEN_INPUT = 100\n",
        "PADDING_LEN_LBL = 20\n",
        "PAD_TOKEN = 50\n",
        "EARLY_STOPPING = 6\n",
        "LOSS_FN = nn.MSELoss(reduction='mean')\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "CnRn4AyWHUeX"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# utility functions\n",
        "\n",
        "The following section includes a few functions that are used later on."
      ],
      "metadata": {
        "id": "XN3SPNk4WVzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate(batch):\n",
        "    event_ids = []\n",
        "    xs, ys, zs = [], [], []\n",
        "    labels = []\n",
        "    labels_pad, lbl_lens = None, None\n",
        "\n",
        "    # load in the elements in the batch\n",
        "    for b in batch:\n",
        "        event_ids.append(b[0])\n",
        "        xs.append(b[1])\n",
        "        ys.append(b[2])\n",
        "        zs.append(b[3])\n",
        "        labels.append(b[4])\n",
        "\n",
        "    x_lens = [len(val) for val in xs]\n",
        "    lbl_lens = [len(lbl) for lbl in labels]\n",
        "\n",
        "    # pad the labels\n",
        "    if DIM == 2:\n",
        "        labels[0] = nn.ConstantPad1d((0, PADDING_LEN_LBL - labels[0].shape[0]), PAD_TOKEN)(labels[0])\n",
        "    if DIM == 3:\n",
        "        labels[0] = nn.ConstantPad2d((0, 0, 0, PADDING_LEN_LBL - labels[0].shape[0]), PAD_TOKEN)(labels[0])\n",
        "    labels_pad = pad_sequence(labels, batch_first=False, padding_value=PAD_TOKEN)\n",
        "\n",
        "    # add padding to the x, y and z vectors\n",
        "    xs[0] = nn.ConstantPad1d((0, PADDING_LEN_INPUT - xs[0].shape[0]), PAD_TOKEN)(xs[0])\n",
        "    ys[0] = nn.ConstantPad1d((0, PADDING_LEN_INPUT - ys[0].shape[0]), PAD_TOKEN)(ys[0])\n",
        "    zs[0] = nn.ConstantPad1d((0, PADDING_LEN_INPUT - zs[0].shape[0]), PAD_TOKEN)(zs[0])\n",
        "\n",
        "    xs_pad = pad_sequence(xs, batch_first=False, padding_value=PAD_TOKEN)\n",
        "    ys_pad = pad_sequence(ys, batch_first=False, padding_value=PAD_TOKEN)\n",
        "    zs_pad = pad_sequence(zs, batch_first=False, padding_value=PAD_TOKEN)\n",
        "    x = torch.stack((xs_pad, ys_pad, zs_pad), dim=1)\n",
        "\n",
        "    # Return the final batch\n",
        "    return event_ids, x.transpose(1, 2), x_lens, labels_pad, lbl_lens\n",
        "\n",
        "\n",
        "def load_variable_len_data(path):\n",
        "    # from https://stackoverflow.com/questions/27020216/import-csv-with-different-number-of-columns-per-row-using-pandas\n",
        "    with open(path, 'r') as f:\n",
        "        col_count = [len(l.split(\",\")) for l in f.readlines()]\n",
        "\n",
        "    # create column names corresponding to their index\n",
        "    column_names = [i for i in range(0, max(col_count))]\n",
        "\n",
        "    # read data with the previously created column names\n",
        "    data = pd.read_csv(path, header=None, delimiter=\",\", names=column_names)\n",
        "    return data\n",
        "\n",
        "\n",
        "def create_mask_src(src, device):\n",
        "    src_seq_len = src.shape[0]\n",
        "    padding_vector = torch.full((src_seq_len,), PAD_TOKEN, device=device)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool)\n",
        "    src_padding_mask = (src.transpose(0, 2) == padding_vector).all(dim=0)\n",
        "\n",
        "    return src_mask, src_padding_mask\n",
        "\n",
        "\n",
        "def create_output_pred_mask(tensor, indices):\n",
        "    indices_arr = np.array(indices)\n",
        "    row_indices = np.arange(tensor.shape[1])[:, np.newaxis]\n",
        "    col_indices = np.arange(tensor.shape[0])\n",
        "    mask = col_indices < indices_arr[row_indices]\n",
        "    return mask.T\n",
        "\n",
        "\n",
        "def get_labels_2d(event_labels, sort=True):\n",
        "    labels = event_labels[2::2]\n",
        "    labels = [float(value) for value in labels if not math.isnan(value)]\n",
        "    if sort:\n",
        "        labels = np.sort(labels)\n",
        "    # print(labels)\n",
        "    return labels\n",
        "\n",
        "\n",
        "def get_labels_3d(event_labels, sort=True):\n",
        "    labels = event_labels[2::2]\n",
        "\n",
        "    tmp_labels = []\n",
        "    for angles in labels:\n",
        "        if not isinstance(angles, float): # if there is a nan, it's a float\n",
        "            angles = angles.split(';')\n",
        "            tmp_labels.append((float(angles[0]), float(angles[1])))\n",
        "    labels = tmp_labels\n",
        "\n",
        "    if sort:\n",
        "        labels = np.sort(labels)\n",
        "    return labels\n",
        "\n",
        "\n",
        "def normalize_data(data):\n",
        "    maximum = data.abs().max()\n",
        "    norm_data = data / maximum\n",
        "    return norm_data\n",
        "\n",
        "def cart2cyl(x, y, z=None):\n",
        "    rho = np.sqrt(x ** 2 + y ** 2)\n",
        "    phi = np.arctan2(y, x)\n",
        "    return (rho, phi, z) if z is not None else (rho, phi)\n",
        "\n",
        "\n",
        "def sort_by_angle(x, y, z = None):\n",
        "    # convert the seperate lists into one list of tuples\n",
        "    coords = []\n",
        "    for i in range(len(x)):\n",
        "            if DIM == 2:\n",
        "                coords.append((x[i], y[i]))\n",
        "            elif DIM == 3:\n",
        "                coords.append((x[i], y[i], z[i]))\n",
        "\n",
        "    # sort the list\n",
        "    dist_coords = np.array(coords)\n",
        "    distances = np.round(np.linalg.norm(dist_coords, axis=1))\n",
        "    # Sort first by rho, round the rho, then sort by phi (sorting by the angle on detector)\n",
        "    cylindrical_coords = [cart2cyl(*coord) for coord in coords]\n",
        "    sorted_indices = np.lexsort((list(zip(*cylindrical_coords))[1], distances))\n",
        "    sorted_cartesian_coords = [coords[i] for i in sorted_indices]\n",
        "\n",
        "    # convert back into seperate lists\n",
        "    if DIM == 2:\n",
        "        x, y = zip(*sorted_cartesian_coords)\n",
        "    elif DIM == 3:\n",
        "        x, y, z = zip(*sorted_cartesian_coords)\n",
        "\n",
        "    return x, y, z\n"
      ],
      "metadata": {
        "id": "dRl2j_-FWU2-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Dataset\n",
        "The following code block defines the dataset which is used by the data loader later on."
      ],
      "metadata": {
        "id": "gypmYH_ekAeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrajectoryDataset(Dataset):\n",
        "    def __init__(self, data_filename, labels_filename, normalize=False):\n",
        "        self.data = load_variable_len_data(data_filename)\n",
        "        self.labels = load_variable_len_data(labels_filename)\n",
        "\n",
        "        self.total_nr_events = len(self.data)\n",
        "        self.normalize = normalize\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_nr_events\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        labels = None\n",
        "        data = self.data.iloc[[idx]].values.tolist()[0]\n",
        "        event_id = int(data[0])\n",
        "\n",
        "        # get the label\n",
        "        event_labels = self.labels.iloc[[event_id]].values.tolist()[0]\n",
        "        if DIM == 2:\n",
        "            labels = get_labels_2d(event_labels)\n",
        "        elif DIM == 3:\n",
        "            labels = get_labels_3d(event_labels)\n",
        "\n",
        "        # get the x and y coordinates\n",
        "        x = data[1::DIM + 1]\n",
        "        y = data[2::DIM + 1]\n",
        "\n",
        "        # convert to float\n",
        "        x = [float(value) for value in x if not math.isnan(value)]\n",
        "        y = [float(value) for value in y if not math.isnan(value)]\n",
        "\n",
        "        # if 2d data we fill the z vector with padding, if 3d data we fill it with data\n",
        "        z = None\n",
        "        if DIM == 2:\n",
        "            z = [PAD_TOKEN] * len(x)\n",
        "        elif DIM == 3:\n",
        "            z = data[3::DIM + 1]\n",
        "            z = [float(value) for value in z if not math.isnan(value)]\n",
        "\n",
        "        # normalise\n",
        "        if self.normalize:\n",
        "            raise NotImplementedError() #TODO\n",
        "\n",
        "        # sort the data\n",
        "        if DIM == 2:\n",
        "            x, y, _ = sort_by_angle(x, y)\n",
        "        elif DIM == 3:\n",
        "            x, y, z = sort_by_angle(x, y, z)\n",
        "\n",
        "        # convert the coordinates and labels to tensors\n",
        "        x = torch.tensor(x).float()\n",
        "        y = torch.tensor(y).float()\n",
        "        z = torch.tensor(z).float()\n",
        "        labels = torch.tensor(labels).float()\n",
        "\n",
        "        # clean up data\n",
        "        del data\n",
        "\n",
        "        return event_id, x, y, z, labels"
      ],
      "metadata": {
        "id": "k_ExMB6VNe7F"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TrajectoryDataset(DATA_FILENAME, LABEL_FILENAME)"
      ],
      "metadata": {
        "id": "EpIBDj92IHtn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Functions\n",
        "The first function is used both for training and evaluating of the model. It gets the data and creates masks, then gets predictions from the model to calculate the loss."
      ],
      "metadata": {
        "id": "ING_DNl6gcN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_eval(model, progress_bar, train=True, optim=None):\n",
        "    losses = 0.\n",
        "    for i, data in progress_bar:\n",
        "        #get the data and labels on the device\n",
        "        _, x, src_len, labels, _ = data\n",
        "        x = x.to(DEVICE)\n",
        "        if labels is not None:\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "        # create masks for the data\n",
        "        src_mask, src_padding_mask = create_mask_src(x, DEVICE)\n",
        "\n",
        "        # run model\n",
        "        pred = model(x, src_mask, src_padding_mask)\n",
        "        if train:\n",
        "            optim.zero_grad()\n",
        "\n",
        "        # create and apply mask to the labels\n",
        "        mask = (labels != PAD_TOKEN).float()\n",
        "        padding_len = np.round(np.divide(src_len, NR_DETECTORS))\n",
        "        labels = labels * mask\n",
        "\n",
        "        # calculate loss for 2d data\n",
        "        if DIM == 2:\n",
        "            pred = pred.transpose(0, 1)\n",
        "            pred_mask = create_output_pred_mask(pred, padding_len)\n",
        "            pred = pred * torch.tensor(pred_mask, device=DEVICE).float()\n",
        "            # loss calculation\n",
        "            loss = LOSS_FN(pred, labels)\n",
        "\n",
        "        # calculate loss for 3d data\n",
        "        elif DIM == 3:\n",
        "            pred = pred[0].transpose(0, 1), pred[1].transpose(0, 1)\n",
        "            pred = torch.stack([pred[0], pred[1]])\n",
        "            for slice_ind in range(pred.shape[0]):\n",
        "                slice_mask = create_output_pred_mask(pred[slice_ind, :, :], padding_len)\n",
        "                pred[slice_ind, :, :] = pred[slice_ind, :, :] * torch.tensor(slice_mask, device=DEVICE).float()\n",
        "            pred = pred.transpose(0, 2)\n",
        "            pred = pred.transpose(1, 0)\n",
        "            # loss calculation\n",
        "            loss = LOSS_FN(pred, labels)\n",
        "\n",
        "        # if training, compute gradients and do backpropagation\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "        # update the progress bar\n",
        "        progress_bar.set_description(\"loss = %.8f\" % loss.item())\n",
        "        losses += loss.item()\n",
        "    return losses"
      ],
      "metadata": {
        "id": "ZyjSRBgDgaMw"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, batch_size, loader):\n",
        "    model.eval()\n",
        "    n_batches = int(math.floor(len(loader.dataset) / batch_size))\n",
        "    progress_bar = tqdm.tqdm(enumerate(loader), total=n_batches)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        losses = train_eval(model, progress_bar, train=False)\n",
        "\n",
        "    return losses / len(loader)"
      ],
      "metadata": {
        "id": "Dug4fiwygoU5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(t_loader, v_loader, transformer, optimizer, batch_size):\n",
        "    train_losses, val_losses = [], []\n",
        "    min_val_loss = np.inf\n",
        "    load = False\n",
        "    epoch, count = 0, 0\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "\n",
        "    for epoch in range(epoch, EPOCHS):\n",
        "        start_time = timer()\n",
        "\n",
        "        # we set the model to train mode and enable gradients\n",
        "        torch.set_grad_enabled(True)\n",
        "        transformer.train()\n",
        "\n",
        "        # we calculate the number of batches for the progress bar\n",
        "        n_batches = int(math.floor(len(t_loader.dataset) / batch_size))\n",
        "        progress_bar = tqdm.tqdm(enumerate(t_loader), total=n_batches)\n",
        "\n",
        "        # we calculate the loss by calling the train_eval function\n",
        "        losses = train_eval(transformer, progress_bar, train=True, optim=optimizer)\n",
        "        train_loss = losses / len(t_loader)\n",
        "\n",
        "        end_time = timer()\n",
        "\n",
        "        # we evaluat the model on the validation set\n",
        "        val_loss = evaluate(transformer, batch_size, v_loader)\n",
        "        print(\"\\nEpoch: \", epoch, \", Train loss: \", train_loss, \\\n",
        "               \", Val loss: \", val_loss, \", Epoch time: \", end_time - start_time, \"\\n\")\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # either saving the best model or the last model and keeping track of early stopping\n",
        "        if val_loss < min_val_loss:\n",
        "            min_val_loss = val_loss\n",
        "            print(\"Saving best model with val_loss: {}\".format(val_loss))\n",
        "            torch.save({'epoch': epoch, 'model_state_dict': transformer.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(), 'train_losses': train_losses,\n",
        "                'val_losses': val_losses,\n",
        "            }, \"model/transformer_encoder_best\")\n",
        "            count = 0\n",
        "        else:\n",
        "            print(\"Saving last model with val_loss: {}\".format(val_loss))\n",
        "            torch.save({'epoch': epoch, 'model_state_dict': transformer.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(), 'train_losses': train_losses,\n",
        "                'val_losses': val_losses,\n",
        "            }, \"model/transformer_encoder_last\")\n",
        "            count += 1\n",
        "\n",
        "        # early stopping criterion\n",
        "        if count >= EARLY_STOPPING:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "    print(\"Final best loss: \", min_val_loss)"
      ],
      "metadata": {
        "id": "pNPSXVTmgtbr"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Model\n",
        "\n",
        "The following class describes the model itself, this code is mostly based on the paper \"Artificial intelligence for improved fitting of trajectories of elementary particles in inhomogeneous dense materials immersed in a magnetic field\". For which code was provided."
      ],
      "metadata": {
        "id": "GdDWbWZyla4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FittingTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,  # number of Transformer encoder layers\n",
        "                 d_model: int,  # length of the new representation\n",
        "                 n_head: int,  # number of heads\n",
        "                 input_size: int,  # size of each item in the input sequence\n",
        "                 output_size: int,  # size of each item in the output sequence\n",
        "                 dim_feedforward: int,  # DIM of the feedforward network of the Transformer\n",
        "                 dropout: float = 0.1,  # dropout value\n",
        "                 seq_len: int = 20):\n",
        "\n",
        "        super(FittingTransformer, self).__init__()\n",
        "        encoder_layers = TransformerEncoderLayer(d_model=d_model,\n",
        "                                                 nhead=n_head,\n",
        "                                                 dim_feedforward=dim_feedforward,\n",
        "                                                 dropout=dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_encoder_layers)\n",
        "        self.proj_input = nn.Linear(input_size, d_model)\n",
        "        self.aggregator = nn.Linear(seq_len, 1)\n",
        "        self.decoder_angle1 = nn.Linear(d_model, output_size)\n",
        "        self.decoder_angle2 = nn.Linear(d_model, output_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self, init_range=0.1) -> None:\n",
        "        # weights initialisation\n",
        "        self.proj_input.bias.data.zero_()\n",
        "        self.proj_input.weight.data.uniform_(-init_range, init_range)\n",
        "        self.decoder_angle1.bias.data.zero_()\n",
        "        self.decoder_angle1.weight.data.uniform_(-init_range, init_range)\n",
        "        self.decoder_angle2.bias.data.zero_()\n",
        "        self.decoder_angle2.weight.data.uniform_(-init_range, init_range)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                mask: Tensor,\n",
        "                src_key_padding_mask: Tensor):\n",
        "\n",
        "        # Linear projection of the input\n",
        "        src_emb = self.proj_input(src)\n",
        "\n",
        "        # Transformer encoder\n",
        "        memory = self.transformer_encoder(src=src_emb, mask=mask,\n",
        "                                          src_key_padding_mask=src_key_padding_mask)\n",
        "        memory = torch.mean(memory, dim=0)\n",
        "\n",
        "        # Dropout\n",
        "        memory = self.dropout(memory)\n",
        "\n",
        "        # Linear projection of the output, with 1 output if there are 2 dimensions and 2 outputs if there are 3 dimensions\n",
        "        if DIM == 2:\n",
        "            output = self.decoder_angle1(memory)\n",
        "            return output\n",
        "        if DIM == 3:\n",
        "            output1 = self.decoder_angle1(memory)\n",
        "            output2 = self.decoder_angle2(memory)\n",
        "            return output1, output2\n"
      ],
      "metadata": {
        "id": "-PyEYdhqNwlm"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the data\n",
        "\n",
        "We define the parameters for the model in order to easily change them for grid search. Then we split the data, create the dataloaders and initialise the model."
      ],
      "metadata": {
        "id": "Am3uBnVklygG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "D_MODEL = 128\n",
        "NUM_ENCODER_LAYERS = 6\n",
        "N_HEAD = 8\n",
        "DIM_FEEDFORWARD = 128\n",
        "DROPOUT = 0.1\n",
        "EPOCHS = 60"
      ],
      "metadata": {
        "id": "oCm6QEQ1VJFj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split data into train=0.6, val=0.2 and test=0.2\n",
        "proportions = [.6, .2, .2]\n",
        "lengths = [int(p * len(dataset)) for p in proportions]\n",
        "lengths[-1] = len(dataset) - sum(lengths[:-1])\n",
        "train_set, val_set, test_set = random_split(dataset, lengths, generator=torch.Generator().manual_seed(123))\n",
        "\n",
        "# create train, validation, and test loaders\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE,\n",
        "                          num_workers=1, shuffle=True, collate_fn=custom_collate)\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE,\n",
        "                          num_workers=1, shuffle=False, collate_fn=custom_collate)\n",
        "test_loader = DataLoader(test_set, batch_size=TEST_BATCH_SIZE,\n",
        "                          num_workers=1, shuffle=False, collate_fn=custom_collate)\n",
        "\n",
        "# Initialise the transformer model\n",
        "transformer = FittingTransformer(num_encoder_layers=NUM_ENCODER_LAYERS,\n",
        "                                d_model=D_MODEL,\n",
        "                                n_head=N_HEAD,\n",
        "                                input_size=3,\n",
        "                                output_size=20,\n",
        "                                dim_feedforward=DIM_FEEDFORWARD,\n",
        "                                dropout=DROPOUT)\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "# print the total number of trainable parameters\n",
        "pytorch_total_params = sum(p.numel() for p in transformer.parameters() if p.requires_grad)\n",
        "print(\"Total number of trainable parameters: {}\".format(pytorch_total_params))\n",
        "\n",
        "# initialise the optimizer\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "CIP2f1dZg4-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69d9c95d-7bc1-4640-e74b-0268ef2dc8d5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of trainable parameters: 603197\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do a small sanity check before training to see if the data is what we expect it to be"
      ],
      "metadata": {
        "id": "F56H0c8tmTP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Length of data: \", len(dataset.data))\n",
        "print(\"Length of labels: \", len(dataset.labels))\n",
        "\n",
        "print(\"Length of train set: \", len(train_set))\n",
        "print(\"Length of val set: \", len(val_set))\n",
        "print(\"Length of test set: \", len(test_set))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuSXlMlmi5sj",
        "outputId": "6afb8c18-e1b4-4cc3-a417-1d42c3098242"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of data:  50000\n",
            "Length of labels:  50000\n",
            "Length of train set:  30000\n",
            "Length of val set:  10000\n",
            "Length of test set:  10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training\n",
        "\n",
        "Here we can finally call the train function."
      ],
      "metadata": {
        "id": "mk0dL0qkhAym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_loader, val_loader, transformer, optimizer, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "T54ych5Ug-lm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7680db5-f9f7-471e-c1ef-47547202720e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss = 0.19905722: : 938it [00:46, 20.38it/s]                       \n",
            "loss = 0.16587153: : 313it [00:12, 24.49it/s]                       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:  0 , Train loss:  0.30813188290100363 , Val loss:  0.17712856964847912 , Epoch time:  46.07615964800061 \n",
            "\n",
            "Saving last model with val_loss: 0.17712856964847912\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "loss = 0.18012981: : 938it [00:46, 20.30it/s]                       \n",
            "loss = 0.13968848: : 313it [00:12, 24.58it/s]                       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:  1 , Train loss:  0.17522346187851576 , Val loss:  0.14608259951344693 , Epoch time:  46.2672115080004 \n",
            "\n",
            "Saving last model with val_loss: 0.14608259951344693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss = 0.11200007: : 938it [00:45, 20.51it/s]                       \n",
            "loss = 0.10121842: : 313it [00:12, 24.35it/s]                       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:  2 , Train loss:  0.15236033006771796 , Val loss:  0.1297816408756442 , Epoch time:  45.77580509100062 \n",
            "\n",
            "Saving last model with val_loss: 0.1297816408756442\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss = 0.11076186: : 938it [00:46, 20.19it/s]                       \n",
            "loss = 0.11302657: : 313it [00:12, 24.49it/s]                       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:  3 , Train loss:  0.12594464999526295 , Val loss:  0.11478321589886571 , Epoch time:  46.48577609200038 \n",
            "\n",
            "Saving last model with val_loss: 0.11478321589886571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss = 0.10502825: : 938it [00:46, 20.25it/s]                       \n",
            "loss = 0.13688496: : 313it [00:12, 24.68it/s]                       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:  4 , Train loss:  0.10745590819574113 , Val loss:  0.11741293026521184 , Epoch time:  46.36335687100018 \n",
            "\n",
            "Saving last model with val_loss: 0.11741293026521184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss = 0.08800628: : 938it [00:45, 20.51it/s]\n",
            "loss = 0.16198783: : 313it [00:12, 24.67it/s]                       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:  5 , Train loss:  0.09605779092925698 , Val loss:  0.13251720461696861 , Epoch time:  45.766459940999994 \n",
            "\n",
            "Saving last model with val_loss: 0.13251720461696861\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss = 0.06565632: : 938it [00:46, 20.26it/s]                       \n",
            "loss = 0.13659783: : 313it [00:12, 24.76it/s]                       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:  6 , Train loss:  0.08702485222838072 , Val loss:  0.10860962025559368 , Epoch time:  46.34022204299981 \n",
            "\n",
            "Saving last model with val_loss: 0.10860962025559368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss = 0.08578198: : 938it [00:45, 20.40it/s]                       \n",
            "loss = 0.18029360: : 313it [00:12, 24.92it/s]                       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:  7 , Train loss:  0.07934440473027067 , Val loss:  0.13937089577936135 , Epoch time:  46.02659069100082 \n",
            "\n",
            "Saving last model with val_loss: 0.13937089577936135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss = 0.06712691: : 938it [00:45, 20.41it/s]                       \n",
            "loss = 0.19890425: : 313it [00:12, 24.90it/s]                       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:  8 , Train loss:  0.07487938146013567 , Val loss:  0.15571078822349968 , Epoch time:  46.01419160000023 \n",
            "\n",
            "Saving last model with val_loss: 0.15571078822349968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss = 0.10242712: : 938it [00:47, 19.71it/s]                       \n",
            "loss = 0.20300360: : 313it [00:12, 24.42it/s]                       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:  9 , Train loss:  0.0691444110085588 , Val loss:  0.1560754012137937 , Epoch time:  47.64256949099945 \n",
            "\n",
            "Saving last model with val_loss: 0.1560754012137937\n",
            "Early stopping\n",
            "Final best loss:  0.10860962025559368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading a saved model\n",
        "\n",
        "If the training has already been done, we can load in a saved model with the following code.\n",
        "\n",
        "The code from setting up the data does have to be ran before this."
      ],
      "metadata": {
        "id": "Bb3d9yy_biNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(\"model/transformer_encoder_last\")\n",
        "transformer.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch'] + 1\n",
        "train_losses = checkpoint['train_losses']\n",
        "val_losses = checkpoint['val_losses']\n",
        "min_val_loss = min(val_losses)\n",
        "print(epoch, val_losses)"
      ],
      "metadata": {
        "id": "4nVHO6oFbhbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating\n",
        "\n",
        "The final step is calculating the loss over the test set."
      ],
      "metadata": {
        "id": "Cid5mgXuhDyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = evaluate(transformer, TEST_BATCH_SIZE, test_loader)\n",
        "print(\"\\ntest loss: \", test_loss)"
      ],
      "metadata": {
        "id": "xKVuV6Ceg_y9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fee8bcd8-96ca-40b7-8a4e-32ad5e1dd39e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss = 0.12862778: 100%|██████████| 5000/5000 [01:04<00:00, 77.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "test loss:  0.1556599835234607\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}